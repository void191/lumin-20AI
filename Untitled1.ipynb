{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnIlSYm4YzoMEWyVJgaJT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/void191/lumin-20AI/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TdEQml0xhVw4",
        "outputId": "408ff1ab-6b00-4b4a-8c52-c976585af60a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Loading summarizer pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load valhalla/t5-small-qa-qg-hl ...\n",
            "Loaded valhalla/t5-small-qa-qg-hl\n",
            "Trying to load lingvanex/kurdish-to-english-translation ...\n",
            "Failed to load lingvanex/kurdish-to-english-translation: <class 'transformers.models.deprecated.van.configuration_van.VanConfig'>\n",
            "Trying to load abdulhade/fine-tuned-MarianMTKurdish ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded abdulhade/fine-tuned-MarianMTKurdish\n",
            "Trying to load lingvanex/kurdish-to-english-translation ...\n",
            "Failed to load lingvanex/kurdish-to-english-translation: <class 'transformers.models.deprecated.van.configuration_van.VanConfig'>\n",
            "Trying to load abdulhade/fine-tuned-MarianMTKurdish ...\n",
            "Loaded abdulhade/fine-tuned-MarianMTKurdish\n",
            "Loaded 1 lessons.\n",
            "FAISS index built.\n",
            "Launching Gradio app...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7866, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# =======================================================\n",
        "# ILE — Full Colab Notebook\n",
        "# Features:\n",
        "# - PDF/TXT lesson ingestion\n",
        "# - Embeddings search (sentence-transformers + FAISS)\n",
        "# - Summarization (transformers pipeline)\n",
        "# - T5 Question Generation (answer-aware)\n",
        "# - Flashcards & cloze generation (NER + noun-chunks)\n",
        "# - Multiple-choice distractors (semantic + heuristic)\n",
        "# - Kurdish detection + translation fallback\n",
        "# - Gradio UI + CSV export\n",
        "# =======================================================\n",
        "\n",
        "# Add this at the top of your notebook, before any NLTK calls:\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # download the missing tokenizer\n",
        "\n",
        "# If you also use the regular sentence tokenizer, you might want:\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# --- Install required libraries ---\n",
        "!pip install --quiet transformers sentence-transformers faiss-cpu PyPDF2 gradio langdetect nltk\n",
        "\n",
        "# --- Imports ---\n",
        "import os, re, csv, random, math\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "import PyPDF2\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM, AutoModel, AutoTokenizer\n",
        "from langdetect import detect, DetectorFactory\n",
        "DetectorFactory.seed = 0\n",
        "import gradio as gr\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import sqlite3\n",
        "import datetime\n",
        "\n",
        "# Connect to local SQLite database\n",
        "conn = sqlite3.connect(\"ile_progress.db\")\n",
        "c = conn.cursor()\n",
        "\n",
        "# Create table if it doesn't exist\n",
        "c.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS progress(\n",
        "    student_id TEXT,\n",
        "    lesson_index INTEGER,\n",
        "    action TEXT,        -- 'summary', 'flashcards', 'quiz'\n",
        "    completed BOOLEAN,\n",
        "    score INTEGER,\n",
        "    timestamp TEXT\n",
        ")\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "\n",
        "\n",
        "# --- Utility: safe model loader for seq2seq models ---\n",
        "def safe_load_seq2seq(model_names):\n",
        "    \"\"\"\n",
        "    Try to load models from a list, return (tokenizer, model, name) for the first that works.\n",
        "    If none work, return (None, None, None).\n",
        "    \"\"\"\n",
        "    for name in model_names:\n",
        "        try:\n",
        "            print(f\"Trying to load {name} ...\")\n",
        "            tok = AutoTokenizer.from_pretrained(name)\n",
        "            mdl = AutoModelForSeq2SeqLM.from_pretrained(name)\n",
        "            print(f\"Loaded {name}\")\n",
        "            return tok, mdl, name\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {name}: {e}\")\n",
        "            continue\n",
        "    return None, None, None\n",
        "\n",
        "# --- Config: model preference lists (can be extended) ---\n",
        "QG_MODELS = [\n",
        "    \"valhalla/t5-small-qa-qg-hl\",            # answer-aware qg (hl tokens)\n",
        "    \"mrm8488/t5-base-finetuned-question-generation-ap\", # t5 fine-tuned for QG\n",
        "    \"iarfmoose/t5-base-question-generator\"\n",
        "]\n",
        "\n",
        "TRANSLATE_KU_TO_EN = [\n",
        "    \"lingvanex/kurdish-to-english-translation\",\n",
        "    \"abdulhade/fine-tuned-MarianMTKurdish\",\n",
        "]\n",
        "TRANSLATE_EN_TO_KU = [\n",
        "    \"lingvanex/kurdish-to-english-translation\",  # many models are bidirectional\n",
        "    \"abdulhade/fine-tuned-MarianMTKurdish\",\n",
        "]\n",
        "\n",
        "# --- Load embeddings model (Sentence-Transformers) ---\n",
        "print(\"Loading embedding model...\")\n",
        "embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  # small & fast\n",
        "\n",
        "# --- Load summarization pipeline (fallback to distilbart) ---\n",
        "print(\"Loading summarizer pipeline...\")\n",
        "try:\n",
        "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "except Exception as e:\n",
        "    print(\"Primary summarizer failed, falling back to facebook/bart-large-cnn:\", e)\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# --- Load QG model (tokenizer+model) ---\n",
        "qg_tokenizer, qg_model, qg_model_name = safe_load_seq2seq(QG_MODELS)\n",
        "if qg_model is None:\n",
        "    print(\"No QG model loaded — question generation will use a simple template fallback.\")\n",
        "\n",
        "# --- Load translation models for Kurdish if available ---\n",
        "ku_en_tok, ku_en_mdl, ku_en_name = safe_load_seq2seq(TRANSLATE_KU_TO_EN)\n",
        "en_ku_tok, en_ku_mdl, en_ku_name = safe_load_seq2seq(TRANSLATE_EN_TO_KU)\n",
        "if ku_en_mdl is None or en_ku_mdl is None:\n",
        "    print(\"Kurdish translation model(s) not fully available — will attempt single-model bidirectional use or skip translation if not possible.\")\n",
        "\n",
        "# --- Helpers: PDF/text extraction ---\n",
        "def extract_pdf_text(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            ptext = page.extract_text()\n",
        "            if ptext:\n",
        "                text += ptext + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# --- Load lessons from 'lessons' folder ---\n",
        "lessons_dir = \"lessons\"\n",
        "Path(lessons_dir).mkdir(exist_ok=True)\n",
        "lesson_titles = []\n",
        "lesson_texts = []\n",
        "for file in sorted(os.listdir(lessons_dir)):\n",
        "    path = os.path.join(lessons_dir, file)\n",
        "    if file.lower().endswith(\".pdf\"):\n",
        "        t = extract_pdf_text(path)\n",
        "        if t.strip():\n",
        "            lesson_titles.append(file)\n",
        "            lesson_texts.append(t)\n",
        "    elif file.lower().endswith(\".txt\"):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
        "            txt = fh.read()\n",
        "            if txt.strip():\n",
        "                lesson_titles.append(file)\n",
        "                lesson_texts.append(txt)\n",
        "if len(lesson_texts) == 0:\n",
        "    print(\"No lessons found in 'lessons' folder. Add PDFs/TXTs and rerun.\")\n",
        "else:\n",
        "    print(f\"Loaded {len(lesson_texts)} lessons.\")\n",
        "\n",
        "# --- Build embeddings + FAISS index if lessons exist ---\n",
        "if len(lesson_texts) > 0:\n",
        "    lesson_embeddings = embed_model.encode(lesson_texts)\n",
        "    lesson_embeddings = np.array(lesson_embeddings).astype('float32')\n",
        "    dim = lesson_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(lesson_embeddings)\n",
        "    print(\"FAISS index built.\")\n",
        "\n",
        "# --- Language detection helper (very lightweight) ---\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except Exception:\n",
        "        return \"en\"\n",
        "\n",
        "# --- Translation helpers (Kurdish <-> English) ---\n",
        "def translate_text(text, src_to_tgt=\"ku2en\"):\n",
        "    \"\"\"\n",
        "    src_to_tgt: \"ku2en\" or \"en2ku\". Uses loaded models if available.\n",
        "    Falls back to returning original text if no translator available.\n",
        "    \"\"\"\n",
        "    if src_to_tgt == \"ku2en\":\n",
        "        if ku_en_mdl is None or ku_en_tok is None:\n",
        "            return text\n",
        "        tok = ku_en_tok\n",
        "        mdl = ku_en_mdl\n",
        "    else:\n",
        "        if en_ku_mdl is None or en_ku_tok is None:\n",
        "            return text\n",
        "        tok = en_ku_tok\n",
        "        mdl = en_ku_mdl\n",
        "    try:\n",
        "        inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        outputs = mdl.generate(**inputs, max_length=1024)\n",
        "        res = tok.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return res[0]\n",
        "    except Exception as e:\n",
        "        print(\"Translation failed:\", e)\n",
        "        return text\n",
        "\n",
        "# --- Summarization utility (chunking for long docs) ---\n",
        "def summarize_text(text, max_chunk_chars=1000):\n",
        "    t = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    if len(t) <= max_chunk_chars:\n",
        "        out = summarizer(t, max_length=120, min_length=30, do_sample=False)[0]['summary_text']\n",
        "        return out\n",
        "    chunks = []\n",
        "    pos = 0\n",
        "    while pos < len(t):\n",
        "        chunk = t[pos: pos + max_chunk_chars]\n",
        "        if pos + max_chunk_chars < len(t):\n",
        "            last = chunk.rfind('. ')\n",
        "            if last != -1:\n",
        "                chunk = chunk[:last+1]\n",
        "        chunks.append(chunk.strip())\n",
        "        pos += len(chunk)\n",
        "    summaries = [summarizer(c, max_length=120, min_length=30, do_sample=False)[0]['summary_text'] for c in chunks]\n",
        "    combined = \" \".join(summaries)\n",
        "    if len(combined) > max_chunk_chars:\n",
        "        combined = summarizer(combined, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
        "    return combined\n",
        "\n",
        "# --- Simple NER and noun-chunk extraction using sentence splitting and heuristics (no heavy Kurdish NER unless KuBERT is used) ---\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    # attempt to download if not present\n",
        "    import os\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_candidates(text, max_cand=200):\n",
        "    doc = nlp(text)\n",
        "    ents = [ent.text.strip() for ent in doc.ents if len(ent.text.strip())>1]\n",
        "    noun_chunks = [nc.text.strip() for nc in doc.noun_chunks if len(nc.text.strip())>2]\n",
        "    cand = []\n",
        "    seen = set()\n",
        "    for item in ents + noun_chunks:\n",
        "        low = item.lower()\n",
        "        if low in seen: continue\n",
        "        seen.add(low)\n",
        "        cand.append(item)\n",
        "        if len(cand) >= max_cand: break\n",
        "    return cand\n",
        "\n",
        "# --- QG: generate question for a given answer span and context using loaded QG model if present ---\n",
        "def generate_question_with_t5(answer, context, max_length=64):\n",
        "    if qg_model is None or qg_tokenizer is None:\n",
        "        # fallback simple template\n",
        "        return f\"What is {answer}?\"\n",
        "    model = qg_model\n",
        "    tok = qg_tokenizer\n",
        "    # some models expect highlight tokens around the answer: <hl> ... <hl>\n",
        "    # create input following patterns used by valhalla/mrm8488 (try multiple patterns)\n",
        "    patterns = [\n",
        "        f\"generate question: {context} </s> answer: {answer}\",            # generic\n",
        "        f\"<hl> {answer} <hl> {context}\",                                  # valhalla style\n",
        "        f\"answer: {answer} context: {context} </s>\"                       # mrm8488 style\n",
        "    ]\n",
        "    for inp in patterns:\n",
        "        try:\n",
        "            inputs = tok.encode(inp, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "            outputs = model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
        "            q = tok.decode(outputs[0], skip_special_tokens=True)\n",
        "            # simple cleanup\n",
        "            q = q.strip()\n",
        "            if len(q) > 5:\n",
        "                return q\n",
        "        except Exception:\n",
        "            continue\n",
        "    # fallback\n",
        "    return f\"What is {answer}?\"\n",
        "\n",
        "# --- Multiple-choice distractor generation ---\n",
        "def generate_mcq(question, answer, context_text, lesson_text, num_distractors=3):\n",
        "    \"\"\"\n",
        "    Strategy:\n",
        "    - Extract candidate distractors from lesson: entities, noun-chunks\n",
        "    - Filter by length & not overlapping answer\n",
        "    - Score semantic similarity via sentence-transformer embeddings: choose candidates that are somewhat close to answer (not too close)\n",
        "    - If not enough, create synthetic distractors by shuffling words or using synonyms heuristic\n",
        "    \"\"\"\n",
        "    candidates = extract_candidates(lesson_text, max_cand=200)\n",
        "    # filter out those that are identical or subsumed in the answer\n",
        "    filt = []\n",
        "    ans_low = answer.lower()\n",
        "    for c in candidates:\n",
        "        c_low = c.lower()\n",
        "        if c_low == ans_low or c_low in ans_low or ans_low in c_low:\n",
        "            continue\n",
        "        if len(c) < 2 or len(c) > 60:\n",
        "            continue\n",
        "        filt.append(c)\n",
        "    # If not enough candidates, also try sentence-level chunks\n",
        "    if len(filt) < num_distractors:\n",
        "        sents = sent_tokenize(lesson_text)\n",
        "        for s in sents:\n",
        "            candidate = \" \".join(s.split()[:6])\n",
        "            if candidate.lower() != ans_low and candidate.lower() not in [f.lower() for f in filt]:\n",
        "                filt.append(candidate)\n",
        "            if len(filt) >= num_distractors: break\n",
        "    # Compute embeddings to pick candidates at medium similarity distance\n",
        "    try:\n",
        "        pool = [answer] + filt\n",
        "        emb = embed_model.encode(pool)\n",
        "        ans_emb = emb[0]\n",
        "        cand_embs = emb[1:]\n",
        "        sims = np.dot(cand_embs, ans_emb) / (np.linalg.norm(cand_embs, axis=1) * (np.linalg.norm(ans_emb)+1e-9))\n",
        "        # Select distractors with mid-range similarity\n",
        "        idxs = np.argsort(-sims)  # descending similarity\n",
        "        chosen = []\n",
        "        for i in idx:\n",
        "            if len(chosen) >= num_distractors: break\n",
        "            # avoid extremely similar (near-duplicates)\n",
        "            if sims[i] > 0.95: continue\n",
        "            chosen.append(filt[i])\n",
        "        # if still short, pad with random candidates\n",
        "        if len(chosen) < num_distractors:\n",
        "            extras = [c for c in filt if c not in chosen]\n",
        "            random.shuffle(extras)\n",
        "            chosen += extras[:(num_distractors-len(chosen))]\n",
        "        # Final fallback: generate word-level distractors\n",
        "        if len(chosen) < num_distractors:\n",
        "            words = [w for w in answer.split() if len(w)>3]\n",
        "            for w in words:\n",
        "                chosen.append(\"\".join(random.sample(w, len(w))))\n",
        "                if len(chosen) >= num_distractors: break\n",
        "        # ensure uniqueness & strip\n",
        "        final = []\n",
        "        for c in chosen:\n",
        "            c = c.strip()\n",
        "            if c.lower() == answer.lower(): continue\n",
        "            if c not in final:\n",
        "                final.append(c)\n",
        "            if len(final) >= num_distractors: break\n",
        "        return final[:num_distractors]\n",
        "    except Exception as e:\n",
        "        # simple random fallback\n",
        "        random.shuffle(filt)\n",
        "        return filt[:num_distractors]\n",
        "\n",
        "# --- Flashcard / QA generation pipeline (combines rule-based + T5 QG) ---\n",
        "def generate_flashcards_and_mcqs(lesson_text, max_cards=20, use_t5_qg=True, language_hint=None):\n",
        "    \"\"\"\n",
        "    Returns a list of items:\n",
        "    {\n",
        "      'type':'short'|'cloze'|'qa',\n",
        "      'answer': ...,\n",
        "      'question': ...,\n",
        "      'distractors': [...],  # optional for MCQ\n",
        "      'context': ...\n",
        "    }\n",
        "    \"\"\"\n",
        "    text = lesson_text\n",
        "    # Detect language if not provided\n",
        "    lang = language_hint or detect_language(text[:200])\n",
        "    translated_to_en = False\n",
        "    if lang.startswith(\"ku\") and (ku_en_mdl is not None):\n",
        "        # translate to English for processing\n",
        "        text_en = translate_text(text, src_to_tgt=\"ku2en\")\n",
        "        translated_to_en = True\n",
        "    else:\n",
        "        text_en = text\n",
        "\n",
        "    # Extract candidates\n",
        "    candidates = extract_candidates(text_en, max_cand=200)\n",
        "    cards = []\n",
        "    seen_answers = set()\n",
        "    # Use entities first for short-answer Qs (try to create Q via T5)\n",
        "    for ent in candidates:\n",
        "        if len(cards) >= max_cards: break\n",
        "        ans = ent\n",
        "        if len(ans.strip()) < 2: continue\n",
        "        low = ans.lower()\n",
        "        if low in seen_answers: continue\n",
        "        seen_answers.add(low)\n",
        "        # find a sentence containing the answer\n",
        "        sents = sent_tokenize(text_en)\n",
        "        ctx = next((s for s in sents if ans in s), sents[0] if sents else text_en[:200])\n",
        "        # generate question by T5 if enabled\n",
        "        if use_t5_qg and qg_model is not None:\n",
        "            q = generate_question_with_t5(ans, ctx)\n",
        "        else:\n",
        "            q = f\"What is {ans}?\"\n",
        "        # generate distractors from lesson\n",
        "        distractors = generate_mcq(q, ans, ctx, text_en, num_distractors=3)\n",
        "        # if we translated, translate back question/answer/distractors to Kurdish\n",
        "        if translated_to_en and en_ku_mdl is not None:\n",
        "            q_local = translate_text(q, src_to_tgt=\"en2ku\")\n",
        "            ans_local = translate_text(ans, src_to_tgt=\"en2ku\")\n",
        "            distractors_local = [translate_text(d, src_to_tgt=\"en2ku\") for d in distractors]\n",
        "        else:\n",
        "            q_local, ans_local, distractors_local = q, ans, distractors\n",
        "        cards.append({\n",
        "            'type':'qa',\n",
        "            'question': q_local,\n",
        "            'answer': ans_local,\n",
        "            'distractors': distractors_local,\n",
        "            'context': ctx\n",
        "        })\n",
        "    # If not enough, generate cloze cards using noun-chunks\n",
        "    if len(cards) < max_cards:\n",
        "        doc = nlp(text_en)\n",
        "        for nc in doc.noun_chunks:\n",
        "            if len(cards) >= max_cards: break\n",
        "            chunk = nc.text.strip()\n",
        "            low = chunk.lower()\n",
        "            if low in seen_answers or len(chunk) < 3: continue\n",
        "            seen_answers.add(low)\n",
        "            sents = sent_tokenize(text_en)\n",
        "            ctx = next((s for s in sents if chunk in s), sents[0] if sents else text_en[:200])\n",
        "            cloze_q = ctx.replace(chunk, \"_____\")\n",
        "            # create simple MCQ distractors\n",
        "            distractors = generate_mcq(cloze_q, chunk, ctx, text_en, num_distractors=3)\n",
        "            if translated_to_en and en_ku_mdl is not None:\n",
        "                q_local = translate_text(cloze_q, src_to_tgt=\"en2ku\")\n",
        "                ans_local = translate_text(chunk, src_to_tgt=\"en2ku\")\n",
        "                distractors_local = [translate_text(d, src_to_tgt=\"en2ku\") for d in distractors]\n",
        "            else:\n",
        "                q_local, ans_local, distractors_local = cloze_q, chunk, distractors\n",
        "            cards.append({\n",
        "                'type':'cloze',\n",
        "                'question': q_local,\n",
        "                'answer': ans_local,\n",
        "                'distractors': distractors_local,\n",
        "                'context': ctx\n",
        "            })\n",
        "    # Final fallback: chunk-based explainers\n",
        "    if len(cards) == 0:\n",
        "        sents = sent_tokenize(text_en)\n",
        "        for i, s in enumerate(sents[:max_cards]):\n",
        "            cards.append({\n",
        "                'type':'short',\n",
        "                'question': f\"Explain: {s[:80]}...\",\n",
        "                'answer': s,\n",
        "                'distractors': [],\n",
        "                'context': s\n",
        "            })\n",
        "    return cards[:max_cards]\n",
        "\n",
        "# --- Export to CSV ---\n",
        "def export_cards_to_csv(cards, filename=None):\n",
        "    if filename is None:\n",
        "        filename = \"ile_flashcards_export.csv\"\n",
        "    out_path = os.path.join(\"/content\", filename)\n",
        "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
        "        writer = csv.writer(fh)\n",
        "        writer.writerow([\"type\", \"question\", \"answer\", \"distractors\", \"context\"])\n",
        "        for c in cards:\n",
        "            writer.writerow([c['type'], c['question'], c['answer'], \"||\".join(c.get('distractors',[])), c.get('context','')])\n",
        "    return out_path, len(cards)\n",
        "\n",
        "# --- Recommendation function (search) ---\n",
        "def recommend_by_query(query, k=3):\n",
        "    if len(lesson_texts)==0:\n",
        "        return []\n",
        "    q_emb = embed_model.encode([query]).astype('float32')\n",
        "    D, I = index.search(q_emb, k)\n",
        "    out = []\n",
        "    for idx in I[0]:\n",
        "        out.append({'index': int(idx), 'title': lesson_titles[idx], 'snippet': lesson_texts[idx][:800]})\n",
        "    return out\n",
        "\n",
        "# --- Gradio UI functions ---\n",
        "def ui_recommend(query):\n",
        "    r = recommend_by_query(query, k=3)\n",
        "    if not r:\n",
        "        return \"No lessons loaded.\"\n",
        "    text = \"\"\n",
        "    for item in r:\n",
        "        text += f\"Index: {item['index']}  | Title: {item['title']}\\nSnippet:\\n{item['snippet']}\\n\\n---\\n\"\n",
        "    return text\n",
        "\n",
        "def ui_summarize(idx):\n",
        "    try:\n",
        "        i = int(idx)\n",
        "        t = lesson_texts[i]\n",
        "        lang = detect_language(t[:200])\n",
        "        if lang.startswith(\"ku\") and ku_en_mdl is not None:\n",
        "            t_en = translate_text(t, src_to_tgt=\"ku2en\")\n",
        "            s_en = summarize_text(t_en)\n",
        "            if en_ku_mdl is not None:\n",
        "                s_local = translate_text(s_en, src_to_tgt=\"en2ku\")\n",
        "                return s_local\n",
        "            else:\n",
        "                return s_en\n",
        "        else:\n",
        "            return summarize_text(t)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def ui_generate(idx, max_cards=10):\n",
        "    try:\n",
        "        i = int(idx)\n",
        "        t = lesson_texts[i]\n",
        "        lang = detect_language(t[:200])\n",
        "        cards = generate_flashcards_and_mcqs(t, max_cards=max_cards, use_t5_qg=True, language_hint=lang)\n",
        "        out = \"\"\n",
        "        for n,c in enumerate(cards):\n",
        "            out += f\"{n+1}. [{c['type']}] Q: {c['question']}\\n   A: {c['answer']}\\n   Distractors: {', '.join(c.get('distractors',[]))}\\n\\n\"\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def ui_export(idx, max_cards=50):\n",
        "    try:\n",
        "        i = int(idx)\n",
        "        t = lesson_texts[i]\n",
        "        lang = detect_language(t[:200])\n",
        "        cards = generate_flashcards_and_mcqs(t, max_cards=max_cards, use_t5_qg=True, language_hint=lang)\n",
        "        path, count = export_cards_to_csv(cards, filename=f\"flashcards_lesson_{i}.csv\")\n",
        "        return f\"Exported {count} cards to {path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# --- Wrapped UI functions with progress logging ---\n",
        "def ui_summarize_with_progress(idx, student_id=\"student1\"):\n",
        "    summary = ui_summarize(idx)  # your existing function\n",
        "    try:\n",
        "        log_progress(student_id, int(idx), action=\"summary\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error logging progress for summarize: {e}\")\n",
        "    return summary\n",
        "\n",
        "def ui_generate_with_progress(idx, max_cards=10, student_id=\"student1\"):\n",
        "    cards_text = ui_generate(idx, max_cards)  # your existing function\n",
        "    try:\n",
        "        log_progress(student_id, int(idx), action=\"flashcards\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error logging progress for flashcards: {e}\")\n",
        "    return cards_text\n",
        "\n",
        "def ui_export_with_progress(idx, max_cards=50, student_id=\"student1\"):\n",
        "    export_status = ui_export(idx, max_cards)  # existing export function\n",
        "    try:\n",
        "        log_progress(student_id, int(idx), action=\"export\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error logging progress for export: {e}\")\n",
        "    return export_status\n",
        "\n",
        "\n",
        "# --- Build Gradio App ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ILE — QG + MCQ + Kurdish support\")\n",
        "    with gr.Row():\n",
        "        student_id = gr.Textbox(label=\"Student ID\", value=\"student1\")\n",
        "    with gr.Row():\n",
        "        q = gr.Textbox(label=\"Search / Ask\", placeholder=\"e.g. 'neural networks intro' or a question\")\n",
        "        btn = gr.Button(\"Recommend Lessons\")\n",
        "        out = gr.Textbox(label=\"Recommendations\", lines=8)\n",
        "    with gr.Row():\n",
        "        idx = gr.Number(value=0, label=\"Lesson index (0-based)\")\n",
        "        sum_btn = gr.Button(\"Summarize Lesson\")\n",
        "        sum_out = gr.Textbox(label=\"Summary\", lines=6)\n",
        "    with gr.Row():\n",
        "        gen_max = gr.Slider(minimum=1, maximum=100, value=10, step=1, label=\"Max cards\")\n",
        "        gen_btn = gr.Button(\"Generate Q/A + MCQs\")\n",
        "        gen_out = gr.Textbox(label=\"Generated Cards\", lines=12)\n",
        "    with gr.Row():\n",
        "        exp_max = gr.Slider(minimum=1, maximum=200, value=50, step=1, label=\"Export max cards\")\n",
        "        exp_btn = gr.Button(\"Export to CSV\")\n",
        "        exp_out = gr.Textbox(label=\"Export status\", lines=2)\n",
        "\n",
        "    btn.click(fn=ui_recommend, inputs=q, outputs=out)\n",
        "    sum_btn.click(fn=ui_summarize_with_progress, inputs=[idx, student_id], outputs=sum_out)\n",
        "    gen_btn.click(fn=ui_generate_with_progress, inputs=[idx, gen_max, student_id], outputs=gen_out)\n",
        "    exp_btn.click(fn=ui_export_with_progress, inputs=[idx, exp_max, student_id], outputs=exp_out)\n",
        "\n",
        "print(\"Launching Gradio app...\")\n",
        "demo.launch(share=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "a2f322e7",
        "outputId": "49295782-635f-44e0-d0d3-0acb36a3fad3"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_path = \"/content/business_basics_1_english_everywhere_by_stephanie_jones_compress.pdf\"\n",
        "lessons_dir = \"lessons\"\n",
        "destination_path = os.path.join(lessons_dir, os.path.basename(source_path))\n",
        "\n",
        "# Create the 'lessons' directory if it doesn't exist\n",
        "os.makedirs(lessons_dir, exist_ok=True)\n",
        "\n",
        "# Move the file\n",
        "shutil.move(source_path, destination_path)\n",
        "\n",
        "print(f\"Moved '{source_path}' to '{destination_path}'\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/business_basics_1_english_everywhere_by_stephanie_jones_compress.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/business_basics_1_english_everywhere_by_stephanie_jones_compress.pdf' -> 'lessons/business_basics_1_english_everywhere_by_stephanie_jones_compress.pdf'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1170945401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Move the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Moved '{source_path}' to '{destination_path}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m             \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/business_basics_1_english_everywhere_by_stephanie_jones_compress.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddc85cf6"
      },
      "source": [
        "def log_progress(student_id, lesson_index, action, completed=True, score=None):\n",
        "    timestamp = datetime.datetime.now().isoformat()\n",
        "    conn = sqlite3.connect(\"ile_progress.db\")\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"INSERT INTO progress VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "              (student_id, lesson_index, action, completed, score if score else 0, timestamp))\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}